Jul 22 02:38:42 mg-cluster-worker2 systemd-journald[90]: Journal started
Jul 22 02:38:42 mg-cluster-worker2 systemd-journald[90]: Runtime Journal (/run/log/journal/8a0345ca3d4c448fb989c501babb6ec5) is 8.0M, max 447.2M, 439.2M free.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-remount-fs.service - Remount Root and Kernel File Systems.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Mounted sys-fs-fuse-connections.mount - FUSE Control File System.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Mounted sys-kernel-config.mount - Kernel Configuration File System.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-firstboot.service - First Boot Wizard was skipped because of an unmet condition check (ConditionFirstBoot=yes).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-journal-flush.service - Flush Journal to Persistent Storage...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-random-seed.service - Load/Save Random Seed was skipped because of an unmet condition check (ConditionVirtualization=!container).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: first-boot-complete.target - First Boot Complete was skipped because of an unmet condition check (ConditionFirstBoot=yes).
Jul 22 02:38:42 mg-cluster-worker2 systemd-journald[90]: Runtime Journal (/run/log/journal/8a0345ca3d4c448fb989c501babb6ec5) is 8.0M, max 447.2M, 439.2M free.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-sysusers.service - Create System Users...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-update-utmp.service - Record System Boot/Shutdown in UTMP...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-modules-load.service - Load Kernel Modules.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-sysctl.service - Apply Kernel Variables...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-journal-flush.service - Flush Journal to Persistent Storage.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-update-utmp.service - Record System Boot/Shutdown in UTMP.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-sysctl.service - Apply Kernel Variables.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-sysusers.service - Create System Users.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-tmpfiles-setup-dev.service - Create Static Device Nodes in /dev...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-tmpfiles-setup-dev.service - Create Static Device Nodes in /dev.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target local-fs-pre.target - Preparation for Local File Systems.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target local-fs.target - Local File Systems.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-machine-id-commit.service - Commit a transient machine-id on disk was skipped because of an unmet condition check (ConditionPathIsMountPoint=/etc/machine-id).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-udevd.service - Rule-based Manager for Device Events and Files was skipped because of an unmet condition check (ConditionPathIsReadWrite=/sys).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target sysinit.target - System Initialization.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Started systemd-tmpfiles-clean.timer - Daily Cleanup of Temporary Directories.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target timers.target - Timer Units.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-pcrphase-sysinit.service - TPM2 PCR Barrier (Initialization) was skipped because of an unmet condition check (ConditionPathExists=/sys/firmware/efi/efivars/StubPcrKernelImage-4a67b082-0a4c-41cf-b6c7-440b29bb8c4f).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target basic.target - Basic System.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-pcrphase.service - TPM2 PCR Barrier (User) was skipped because of an unmet condition check (ConditionPathExists=/sys/firmware/efi/efivars/StubPcrKernelImage-4a67b082-0a4c-41cf-b6c7-440b29bb8c4f).
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting undo-mount-hacks.service - Undo KIND mount hacks...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: undo-mount-hacks.service: Deactivated successfully.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished undo-mount-hacks.service - Undo KIND mount hacks.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting containerd.service - containerd container runtime...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: kubelet.service - kubelet: The Kubernetes Node Agent was skipped because of an unmet condition check (ConditionPathExists=/var/lib/kubelet/config.yaml).
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.306047626Z" level=info msg="starting containerd" revision=926c9586fe4a6236699318391cd44976a98e31f1 version=v1.7.15
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.323947834Z" level=info msg="loading plugin \"io.containerd.event.v1.exchange\"..." type=io.containerd.event.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324030418Z" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324098918Z" level=info msg="loading plugin \"io.containerd.warning.v1.deprecations\"..." type=io.containerd.warning.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324126668Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.blockfile\"..." type=io.containerd.snapshotter.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324145334Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.blockfile\"..." error="no scratch file generator: skip plugin" type=io.containerd.snapshotter.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324152668Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324164751Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324235584Z" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324249043Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.fuse-overlayfs\"..." type=io.containerd.snapshotter.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324574001Z" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.324753459Z" level=info msg="metadata content store policy set" policy=shared
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325003084Z" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325022584Z" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325031084Z" level=info msg="loading plugin \"io.containerd.lease.v1.manager\"..." type=io.containerd.lease.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325039293Z" level=info msg="loading plugin \"io.containerd.streaming.v1.manager\"..." type=io.containerd.streaming.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325048876Z" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325076668Z" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325182876Z" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325217043Z" level=info msg="loading plugin \"io.containerd.runtime.v2.shim\"..." type=io.containerd.runtime.v2
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325223834Z" level=info msg="loading plugin \"io.containerd.sandbox.store.v1.local\"..." type=io.containerd.sandbox.store.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325230626Z" level=info msg="loading plugin \"io.containerd.sandbox.controller.v1.local\"..." type=io.containerd.sandbox.controller.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325237293Z" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325245126Z" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325251334Z" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325257459Z" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325264043Z" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325270126Z" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325279376Z" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325284751Z" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325295001Z" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325304959Z" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325310876Z" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325317376Z" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325323668Z" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325330418Z" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325336918Z" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325343126Z" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325348876Z" level=info msg="loading plugin \"io.containerd.grpc.v1.sandbox-controllers\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325356168Z" level=info msg="loading plugin \"io.containerd.grpc.v1.sandboxes\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325362001Z" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325367376Z" level=info msg="loading plugin \"io.containerd.grpc.v1.streaming\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325373251Z" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325380334Z" level=info msg="loading plugin \"io.containerd.transfer.v1.local\"..." type=io.containerd.transfer.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325393459Z" level=info msg="loading plugin \"io.containerd.grpc.v1.transfer\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325399543Z" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325404626Z" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325451834Z" level=info msg="loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." type=io.containerd.tracing.processor.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325460543Z" level=info msg="skip loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." error="no OpenTelemetry endpoint: skip plugin" type=io.containerd.tracing.processor.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325466293Z" level=info msg="loading plugin \"io.containerd.internal.v1.tracing\"..." type=io.containerd.internal.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325471084Z" level=info msg="skipping tracing processor initialization (no tracing plugin)" error="no OpenTelemetry endpoint: skip plugin"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325506584Z" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325512751Z" level=info msg="loading plugin \"io.containerd.nri.v1.nri\"..." type=io.containerd.nri.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325525918Z" level=info msg="NRI interface is disabled by configuration."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325531418Z" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.325863918Z" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false PrivilegedWithoutHostDevicesAllDevicesAllowed:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0 Snapshotter: SandboxMode:} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false PrivilegedWithoutHostDevicesAllDevicesAllowed:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0 Snapshotter: SandboxMode:} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false PrivilegedWithoutHostDevicesAllDevicesAllowed:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0 Snapshotter: SandboxMode:podsandbox} test-handler:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false PrivilegedWithoutHostDevicesAllDevicesAllowed:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0 Snapshotter: SandboxMode:podsandbox}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:true IgnoreBlockIONotEnabledErrors:false IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginSetupSerially:false NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath: Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:registry.k8s.io/pause:3.7 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false EnableCDI:false CDISpecDirs:[/etc/cdi /var/run/cdi] ImagePullProgressTimeout:5m0s DrainExecSyncIOTimeout:0s ImagePullWithSyncFs:false IgnoreDeprecationWarnings:[]} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.326177918Z" level=info msg="Connect containerd service"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.326210543Z" level=info msg="using legacy CRI server"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.326220209Z" level=info msg="using experimental NRI integration - disable nri plugin to prevent this"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.326251543Z" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.326560293Z" level=error msg="failed to load cni during init, please check CRI plugin status before setting up network for pods" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.328266376Z" level=info msg="Start subscribing containerd event"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.328294459Z" level=info msg="Start recovering state"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.328426334Z" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.328468501Z" level=info msg=serving... address=/run/containerd/containerd.sock
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.328947501Z" level=warning msg="The image sha256:e5a475a0380575fb5df454b2e32bdec93e1ec0094d8a61e895b41567cb884550 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.329439126Z" level=warning msg="The image docker.io/kindest/kindnetd:v20240202-8f1494ea is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.329770668Z" level=warning msg="The image docker.io/kindest/local-path-helper:v20230510-486859a6 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.330537709Z" level=warning msg="The image docker.io/kindest/local-path-provisioner:v20240202-8f1494ea is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.330776626Z" level=warning msg="The image import-2024-05-13@sha256:45c561194466a77e2ad957a3ead7e40af710be745eab60aa556e670cc526e1e9 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.331967626Z" level=warning msg="The image import-2024-05-13@sha256:b7267730b5d235ec9500112663c8b244e2d09f0d9f92f1c6bb5caadf73d7a90c is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.333246334Z" level=warning msg="The image sha256:4740c1948d3fceb8d7dacc63033aa6299d80794ee4f4811539ec1081d9211f3d is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.333443793Z" level=warning msg="The image registry.k8s.io/kube-apiserver-arm64:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.333601834Z" level=warning msg="The image sha256:d2d4e1917462f2edff8cdb18781ff486c0d0c8c3f9f9579654bd14777d0c0b04 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.334095626Z" level=warning msg="The image registry.k8s.io/kube-controller-manager-arm64:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.335894043Z" level=warning msg="The image sha256:26ceefab31d20e7e9c162db69ac8d4bf87f35302f62c2e25a6ffa4ce0fbc5b53 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.336784793Z" level=warning msg="The image registry.k8s.io/kube-scheduler:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.336962209Z" level=warning msg="The image registry.k8s.io/kube-scheduler-arm64:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.337720126Z" level=warning msg="The image registry.k8s.io/pause:3.7 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.337985293Z" level=warning msg="The image sha256:2437cf762177702dec2dfe99a09c37427a15af6d9a57c456b65352667c223d93 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.339578251Z" level=warning msg="The image sha256:b1522b854947b7e938a4fd8b70afad16fcd09e70a373fb7aaa0bdd89ca67bfcc is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.341140001Z" level=warning msg="The image import-2024-05-13@sha256:91ca1c81e44a332253b9588418418de977942a50ae0d9a84f9a288c4030f8653 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.342998959Z" level=warning msg="The image registry.k8s.io/coredns/coredns:v1.11.1 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.343197459Z" level=warning msg="The image sha256:6c806bad1e8991c6415cfcc6f4d01ac610635ff47c9f4c21092caa830d90aca1 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.343397126Z" level=warning msg="The image sha256:80a7c4a0557e68d4257ff8126260f000970de15b987ea4c4eba74506dae713a7 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.343514418Z" level=warning msg="The image registry.k8s.io/kube-proxy-arm64:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.343887918Z" level=warning msg="The image registry.k8s.io/kube-controller-manager:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.344309251Z" level=warning msg="The image import-2024-05-13@sha256:66851d132bc433174ca3ded1bccbfe461cb7766bfd941f956224eafc2c8ec39f is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.344502543Z" level=warning msg="The image sha256:014faa467e29798aeef733fe6d1a3b5e382688217b053ad23410e6cccd5d22fd is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.344567543Z" level=warning msg="The image sha256:d022557af8b6306cc24be6f095d77b7892f28b34b765c4337774e5e4cbb39132 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.344781126Z" level=warning msg="The image registry.k8s.io/etcd:3.5.12-0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.344890334Z" level=warning msg="The image registry.k8s.io/kube-proxy:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.354730668Z" level=warning msg="The image registry.k8s.io/kube-apiserver:v1.30.0 is not unpacked."
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.364443459Z" level=info msg="Start event monitor"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.364912709Z" level=info msg="Start snapshots syncer"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.364951501Z" level=info msg="Start cni network conf syncer for default"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.365128918Z" level=info msg="Start streaming server"
Jul 22 02:38:42 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:38:42.365478543Z" level=info msg="containerd successfully booted in 0.059589s"
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Started containerd.service - containerd container runtime.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target multi-user.target - Multi-User System.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Reached target graphical.target - Graphical Interface.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Starting systemd-update-utmp-runlevel.service - Record Runlevel Change in UTMP...
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: systemd-update-utmp-runlevel.service: Deactivated successfully.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Finished systemd-update-utmp-runlevel.service - Record Runlevel Change in UTMP.
Jul 22 02:38:42 mg-cluster-worker2 systemd[1]: Startup finished in 280ms.
Jul 22 02:39:12 mg-cluster-worker2 systemd[1]: Reloading.
Jul 22 02:39:12 mg-cluster-worker2 systemd[1]: Starting kubelet.service - kubelet: The Kubernetes Node Agent...
Jul 22 02:39:12 mg-cluster-worker2 systemd[1]: Started kubelet.service - kubelet: The Kubernetes Node Agent.
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.014305     175 server.go:205] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.300517     175 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.300539     175 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.300784     175 server.go:927] "Client rotation is on, will bootstrap in background"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.302187     175 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.315814     175 server.go:810] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.315971     175 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=["kubelet"]
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.315996     175 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"mg-cluster-worker2","RuntimeCgroupsName":"/system.slice/containerd.service","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/kubelet","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316133     175 topology_manager.go:138] "Creating topology manager with none policy"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316139     175 container_manager_linux.go:301] "Creating device plugin manager"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316200     175 state_mem.go:36] "Initialized new in-memory state store"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316255     175 kubelet.go:400] "Attempting to sync node with API server"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316271     175 kubelet.go:301] "Adding static pod path" path="/etc/kubernetes/manifests"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316283     175 kubelet.go:312] "Adding apiserver pod source"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316290     175 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.316987     175 kuberuntime_manager.go:261] "Container runtime initialized" containerRuntime="containerd" version="v1.7.15" apiVersion="v1"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.317110     175 kubelet.go:815] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: W0722 02:39:13.317167     175 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.318862     175 server.go:1264] "Started kubelet"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.318951     175 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.318979     175 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.319117     175 server.go:227] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.319436     175 server.go:455] "Adding debug handlers to kubelet server"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.320063     175 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.320303     175 volume_manager.go:291] "Starting Kubelet Volume Manager"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.320348     175 desired_state_of_world_populator.go:149] "Desired state populator starts to run"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.320379     175 reconciler.go:26] "Reconciler: start to sync state"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.322077     175 factory.go:221] Registration of the systemd container factory successfully
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.322169     175 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.327504     175 factory.go:221] Registration of the containerd container factory successfully
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.330928     175 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:anonymous\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" event="&Event{ObjectMeta:{mg-cluster-worker2.17e4689689372fef  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:mg-cluster-worker2,UID:mg-cluster-worker2,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:mg-cluster-worker2,},FirstTimestamp:2024-07-22 02:39:13.318850543 +0000 UTC m=+0.353871876,LastTimestamp:2024-07-22 02:39:13.318850543 +0000 UTC m=+0.353871876,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:mg-cluster-worker2,}"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.331105     175 controller.go:145] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s.io \"mg-cluster-worker2\" is forbidden: User \"system:anonymous\" cannot get resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"kube-node-lease\"" interval="200ms"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: W0722 02:39:13.331132     175 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes "mg-cluster-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.331146     175 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes "mg-cluster-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: W0722 02:39:13.331164     175 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.331169     175 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: W0722 02:39:13.331220     175 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.331227     175 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.337251     175 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.338204     175 cpu_manager.go:214] "Starting CPU manager" policy="none"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.338294     175 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.338308     175 state_mem.go:36] "Initialized new in-memory state store"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.339648     175 policy_none.go:49] "None policy: Start"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.340099     175 memory_manager.go:170] "Starting memorymanager" policy="None"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.340139     175 state_mem.go:35] "Initializing new in-memory state store"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.340835     175 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.340869     175 status_manager.go:217] "Starting to sync pod status with apiserver"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: I0722 02:39:13.340882     175 kubelet.go:2337] "Starting kubelet main sync loop"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.340904     175 kubelet.go:2361] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.345300     175 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:anonymous\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" event="&Event{ObjectMeta:{mg-cluster-worker2.17e468968a5675c3  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:mg-cluster-worker2,UID:mg-cluster-worker2,APIVersion:,ResourceVersion:,FieldPath:,},Reason:NodeHasSufficientMemory,Message:Node mg-cluster-worker2 status is now: NodeHasSufficientMemory,Source:EventSource{Component:kubelet,Host:mg-cluster-worker2,},FirstTimestamp:2024-07-22 02:39:13.337677251 +0000 UTC m=+0.372698626,LastTimestamp:2024-07-22 02:39:13.337677251 +0000 UTC m=+0.372698626,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:mg-cluster-worker2,}"
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: W0722 02:39:13.345862     175 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.345939     175 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.356463     175 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:anonymous\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" event="&Event{ObjectMeta:{mg-cluster-worker2.17e468968a568128  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:mg-cluster-worker2,UID:mg-cluster-worker2,APIVersion:,ResourceVersion:,FieldPath:,},Reason:NodeHasNoDiskPressure,Message:Node mg-cluster-worker2 status is now: NodeHasNoDiskPressure,Source:EventSource{Component:kubelet,Host:mg-cluster-worker2,},FirstTimestamp:2024-07-22 02:39:13.337680168 +0000 UTC m=+0.372701501,LastTimestamp:2024-07-22 02:39:13.337680168 +0000 UTC m=+0.372701501,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:mg-cluster-worker2,}"
Jul 22 02:39:13 mg-cluster-worker2 systemd[1]: Created slice kubelet-kubepods.slice - libcontainer container kubelet-kubepods.slice.
Jul 22 02:39:13 mg-cluster-worker2 kubelet[175]: E0722 02:39:13.370848     175 kubelet.go:1547] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubelet kubepods] doesn't exist"
Jul 22 02:39:13 mg-cluster-worker2 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jul 22 02:39:13 mg-cluster-worker2 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: Stopped kubelet.service - kubelet: The Kubernetes Node Agent.
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: Starting kubelet.service - kubelet: The Kubernetes Node Agent...
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: Started kubelet.service - kubelet: The Kubernetes Node Agent.
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.510846     215 server.go:205] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.513519     215 server.go:484] "Kubelet version" kubeletVersion="v1.30.0"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.513762     215 server.go:486] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.513910     215 server.go:927] "Client rotation is on, will bootstrap in background"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.514223     215 bootstrap.go:241] unable to read existing bootstrap client config from /etc/kubernetes/kubelet.conf: invalid configuration: [unable to read client-cert /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory, unable to read client-key /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory]
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.516729     215 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.524470     215 server.go:810] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.524845     215 container_manager_linux.go:265] "Container manager verified user specified cgroup-root exists" cgroupRoot=["kubelet"]
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.524894     215 container_manager_linux.go:270] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"mg-cluster-worker2","RuntimeCgroupsName":"/system.slice/containerd.service","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/kubelet","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null}
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525066     215 topology_manager.go:138] "Creating topology manager with none policy"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525096     215 container_manager_linux.go:301] "Creating device plugin manager"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525136     215 state_mem.go:36] "Initialized new in-memory state store"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525200     215 kubelet.go:400] "Attempting to sync node with API server"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525227     215 kubelet.go:301] "Adding static pod path" path="/etc/kubernetes/manifests"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525271     215 kubelet.go:312] "Adding apiserver pod source"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.525691     215 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.529227     215 kuberuntime_manager.go:261] "Container runtime initialized" containerRuntime="containerd" version="v1.7.15" apiVersion="v1"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.529406     215 kubelet.go:815] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.529811     215 server.go:1264] "Started kubelet"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.531158     215 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.532429     215 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.533391     215 server.go:455] "Adding debug handlers to kubelet server"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.533950     215 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.536265     215 server.go:227] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.536901     215 volume_manager.go:291] "Starting Kubelet Volume Manager"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.539509     215 desired_state_of_world_populator.go:149] "Desired state populator starts to run"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.537374     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.539987     215 reconciler.go:26] "Reconciler: start to sync state"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.540896     215 factory.go:221] Registration of the systemd container factory successfully
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.540988     215 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.541492     215 controller.go:145] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s.io \"mg-cluster-worker2\" is forbidden: User \"system:anonymous\" cannot get resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"kube-node-lease\"" interval="200ms"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: W0722 02:39:14.541609     215 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.541647     215 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: W0722 02:39:14.541701     215 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes "mg-cluster-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.541789     215 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes "mg-cluster-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: W0722 02:39:14.541822     215 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.541970     215 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.542119     215 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:anonymous\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" event="&Event{ObjectMeta:{mg-cluster-worker2.17e46896d164d08f  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:mg-cluster-worker2,UID:mg-cluster-worker2,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:mg-cluster-worker2,},FirstTimestamp:2024-07-22 02:39:14.529800335 +0000 UTC m=+0.048555584,LastTimestamp:2024-07-22 02:39:14.529800335 +0000 UTC m=+0.048555584,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:mg-cluster-worker2,}"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.542662     215 factory.go:221] Registration of the containerd container factory successfully
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553025     215 cpu_manager.go:214] "Starting CPU manager" policy="none"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553179     215 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553234     215 state_mem.go:36] "Initialized new in-memory state store"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553357     215 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553385     215 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.553419     215 policy_none.go:49] "None policy: Start"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.554526     215 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:anonymous\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" event="&Event{ObjectMeta:{mg-cluster-worker2.17e46896d2bcc575  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:mg-cluster-worker2,UID:mg-cluster-worker2,APIVersion:,ResourceVersion:,FieldPath:,},Reason:NodeHasSufficientMemory,Message:Node mg-cluster-worker2 status is now: NodeHasSufficientMemory,Source:EventSource{Component:kubelet,Host:mg-cluster-worker2,},FirstTimestamp:2024-07-22 02:39:14.552341877 +0000 UTC m=+0.071097126,LastTimestamp:2024-07-22 02:39:14.552341877 +0000 UTC m=+0.071097126,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:mg-cluster-worker2,}"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.555383     215 memory_manager.go:170] "Starting memorymanager" policy="None"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.555442     215 state_mem.go:35] "Initializing new in-memory state store"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.555650     215 state_mem.go:75] "Updated machine memory state"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.558658     215 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.561233     215 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.561257     215 status_manager.go:217] "Starting to sync pod status with apiserver"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.561291     215 kubelet.go:2337] "Starting kubelet main sync loop"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.561312     215 kubelet.go:2361] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: Created slice kubelet-kubepods-burstable.slice - libcontainer container kubelet-kubepods-burstable.slice.
Jul 22 02:39:14 mg-cluster-worker2 systemd[1]: Created slice kubelet-kubepods-besteffort.slice - libcontainer container kubelet-kubepods-besteffort.slice.
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.575385     215 manager.go:479] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.575445     215 container_log_manager.go:186] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.579120     215 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.581448     215 eviction_manager.go:282] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"mg-cluster-worker2\" not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.641026     215 kubelet_node_status.go:73] "Attempting to register node" node="mg-cluster-worker2"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: I0722 02:39:14.648746     215 kubelet_node_status.go:76] "Successfully registered node" node="mg-cluster-worker2"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.656800     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.757733     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.858821     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:14 mg-cluster-worker2 kubelet[215]: E0722 02:39:14.959751     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.061186     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.162157     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.262940     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.363106     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.464073     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: I0722 02:39:15.517388     215 transport.go:147] "Certificate rotation detected, shutting down client connections to start using new credentials"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: W0722 02:39:15.517484     215 reflector.go:470] k8s.io/client-go/informers/factory.go:160: watch of *v1.RuntimeClass ended with: very short watch: k8s.io/client-go/informers/factory.go:160: Unexpected watch close - watch lasted less than a second and no items received
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.564791     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.664971     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.765515     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.866254     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:15 mg-cluster-worker2 kubelet[215]: E0722 02:39:15.967227     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: E0722 02:39:16.067633     215 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"mg-cluster-worker2\" not found"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.169897     215 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.2.0/24"
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.170130753Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.170245     215 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.2.0/24"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.527066     215 apiserver.go:52] "Watching apiserver"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.529122     215 topology_manager.go:215] "Topology Admit Handler" podUID="ab683233-59f5-4e62-ab22-8c7a193d50ed" podNamespace="kube-system" podName="kindnet-bwfhf"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.529194     215 topology_manager.go:215] "Topology Admit Handler" podUID="f4772298-b353-4dc6-9963-ad1d8ecc2f4c" podNamespace="kube-system" podName="kube-proxy-vn9xb"
Jul 22 02:39:16 mg-cluster-worker2 systemd[1]: Created slice kubelet-kubepods-besteffort-podf4772298_b353_4dc6_9963_ad1d8ecc2f4c.slice - libcontainer container kubelet-kubepods-besteffort-podf4772298_b353_4dc6_9963_ad1d8ecc2f4c.slice.
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.539964     215 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Jul 22 02:39:16 mg-cluster-worker2 systemd[1]: Created slice kubelet-kubepods-podab683233_59f5_4e62_ab22_8c7a193d50ed.slice - libcontainer container kubelet-kubepods-podab683233_59f5_4e62_ab22_8c7a193d50ed.slice.
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551785     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/ab683233-59f5-4e62-ab22-8c7a193d50ed-cni-cfg\") pod \"kindnet-bwfhf\" (UID: \"ab683233-59f5-4e62-ab22-8c7a193d50ed\") " pod="kube-system/kindnet-bwfhf"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551815     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ab683233-59f5-4e62-ab22-8c7a193d50ed-xtables-lock\") pod \"kindnet-bwfhf\" (UID: \"ab683233-59f5-4e62-ab22-8c7a193d50ed\") " pod="kube-system/kindnet-bwfhf"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551829     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ab683233-59f5-4e62-ab22-8c7a193d50ed-lib-modules\") pod \"kindnet-bwfhf\" (UID: \"ab683233-59f5-4e62-ab22-8c7a193d50ed\") " pod="kube-system/kindnet-bwfhf"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551841     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x9p9l\" (UniqueName: \"kubernetes.io/projected/ab683233-59f5-4e62-ab22-8c7a193d50ed-kube-api-access-x9p9l\") pod \"kindnet-bwfhf\" (UID: \"ab683233-59f5-4e62-ab22-8c7a193d50ed\") " pod="kube-system/kindnet-bwfhf"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551851     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/f4772298-b353-4dc6-9963-ad1d8ecc2f4c-kube-proxy\") pod \"kube-proxy-vn9xb\" (UID: \"f4772298-b353-4dc6-9963-ad1d8ecc2f4c\") " pod="kube-system/kube-proxy-vn9xb"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551859     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f4772298-b353-4dc6-9963-ad1d8ecc2f4c-xtables-lock\") pod \"kube-proxy-vn9xb\" (UID: \"f4772298-b353-4dc6-9963-ad1d8ecc2f4c\") " pod="kube-system/kube-proxy-vn9xb"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551866     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f4772298-b353-4dc6-9963-ad1d8ecc2f4c-lib-modules\") pod \"kube-proxy-vn9xb\" (UID: \"f4772298-b353-4dc6-9963-ad1d8ecc2f4c\") " pod="kube-system/kube-proxy-vn9xb"
Jul 22 02:39:16 mg-cluster-worker2 kubelet[215]: I0722 02:39:16.551883     215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w9bx2\" (UniqueName: \"kubernetes.io/projected/f4772298-b353-4dc6-9963-ad1d8ecc2f4c-kube-api-access-w9bx2\") pod \"kube-proxy-vn9xb\" (UID: \"f4772298-b353-4dc6-9963-ad1d8ecc2f4c\") " pod="kube-system/kube-proxy-vn9xb"
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.843527920Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-vn9xb,Uid:f4772298-b353-4dc6-9963-ad1d8ecc2f4c,Namespace:kube-system,Attempt:0,}"
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.848510587Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-bwfhf,Uid:ab683233-59f5-4e62-ab22-8c7a193d50ed,Namespace:kube-system,Attempt:0,}"
Jul 22 02:39:16 mg-cluster-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount2856434084.mount: Deactivated successfully.
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.891014753Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.891079712Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.891088628Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.891157295Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.899919087Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.899979003Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.900166628Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.900261087Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 22 02:39:16 mg-cluster-worker2 systemd[1]: Started cri-containerd-e65ed5b2a559b98299bdcfa161c2d4487153518887e54e3d2f22f299c3e865a1.scope - libcontainer container e65ed5b2a559b98299bdcfa161c2d4487153518887e54e3d2f22f299c3e865a1.
Jul 22 02:39:16 mg-cluster-worker2 systemd[1]: Started cri-containerd-8828979bf759d27ef42ef8e3e27bb0b53edd210f0b8ea15dfc356f61f47dd9e5.scope - libcontainer container 8828979bf759d27ef42ef8e3e27bb0b53edd210f0b8ea15dfc356f61f47dd9e5.
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.940507878Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-vn9xb,Uid:f4772298-b353-4dc6-9963-ad1d8ecc2f4c,Namespace:kube-system,Attempt:0,} returns sandbox id \"e65ed5b2a559b98299bdcfa161c2d4487153518887e54e3d2f22f299c3e865a1\""
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.943405295Z" level=info msg="CreateContainer within sandbox \"e65ed5b2a559b98299bdcfa161c2d4487153518887e54e3d2f22f299c3e865a1\" for container &ContainerMetadata{Name:kube-proxy,Attempt:0,}"
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.952440087Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-bwfhf,Uid:ab683233-59f5-4e62-ab22-8c7a193d50ed,Namespace:kube-system,Attempt:0,} returns sandbox id \"8828979bf759d27ef42ef8e3e27bb0b53edd210f0b8ea15dfc356f61f47dd9e5\""
Jul 22 02:39:16 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:16.955963545Z" level=info msg="CreateContainer within sandbox \"8828979bf759d27ef42ef8e3e27bb0b53edd210f0b8ea15dfc356f61f47dd9e5\" for container &ContainerMetadata{Name:kindnet-cni,Attempt:0,}"
Jul 22 02:39:17 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:17.327179503Z" level=info msg="CreateContainer within sandbox \"e65ed5b2a559b98299bdcfa161c2d4487153518887e54e3d2f22f299c3e865a1\" for &ContainerMetadata{Name:kube-proxy,Attempt:0,} returns container id \"b998b3ceaabb4ca633e3d325843e89a0e19c0da2501ae8adf911e7073859da09\""
Jul 22 02:39:17 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:17.327952920Z" level=info msg="StartContainer for \"b998b3ceaabb4ca633e3d325843e89a0e19c0da2501ae8adf911e7073859da09\""
Jul 22 02:39:17 mg-cluster-worker2 systemd[1]: Started cri-containerd-b998b3ceaabb4ca633e3d325843e89a0e19c0da2501ae8adf911e7073859da09.scope - libcontainer container b998b3ceaabb4ca633e3d325843e89a0e19c0da2501ae8adf911e7073859da09.
Jul 22 02:39:17 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:17.441158962Z" level=info msg="StartContainer for \"b998b3ceaabb4ca633e3d325843e89a0e19c0da2501ae8adf911e7073859da09\" returns successfully"
Jul 22 02:39:17 mg-cluster-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount3567680925.mount: Deactivated successfully.
Jul 22 02:39:17 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:17.833048754Z" level=info msg="CreateContainer within sandbox \"8828979bf759d27ef42ef8e3e27bb0b53edd210f0b8ea15dfc356f61f47dd9e5\" for &ContainerMetadata{Name:kindnet-cni,Attempt:0,} returns container id \"74e3e47a9080f8e5e9884f8fed9f6683a898807644af101dcb651121c027a100\""
Jul 22 02:39:17 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:17.833642462Z" level=info msg="StartContainer for \"74e3e47a9080f8e5e9884f8fed9f6683a898807644af101dcb651121c027a100\""
Jul 22 02:39:17 mg-cluster-worker2 systemd[1]: Started cri-containerd-74e3e47a9080f8e5e9884f8fed9f6683a898807644af101dcb651121c027a100.scope - libcontainer container 74e3e47a9080f8e5e9884f8fed9f6683a898807644af101dcb651121c027a100.
Jul 22 02:39:18 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:18.235956420Z" level=info msg="StartContainer for \"74e3e47a9080f8e5e9884f8fed9f6683a898807644af101dcb651121c027a100\" returns successfully"
Jul 22 02:39:18 mg-cluster-worker2 kubelet[215]: I0722 02:39:18.585426     215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-vn9xb" podStartSLOduration=4.585414796 podStartE2EDuration="4.585414796s" podCreationTimestamp="2024-07-22 02:39:14 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-07-22 02:39:17.582450087 +0000 UTC m=+3.101205336" watchObservedRunningTime="2024-07-22 02:39:18.585414796 +0000 UTC m=+4.104170044"
Jul 22 02:39:18 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:18.724503879Z" level=error msg="failed to reload cni configuration after receiving fs change event(WRITE         \"/etc/cni/net.d/10-kindnet.conflist.temp\")" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Jul 22 02:39:18 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:18.726844921Z" level=error msg="failed to reload cni configuration after receiving fs change event(WRITE         \"/etc/cni/net.d/10-kindnet.conflist.temp\")" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Jul 22 02:39:18 mg-cluster-worker2 containerd[105]: time="2024-07-22T02:39:18.726904921Z" level=error msg="failed to reload cni configuration after receiving fs change event(WRITE         \"/etc/cni/net.d/10-kindnet.conflist.temp\")" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Jul 22 02:39:18 mg-cluster-worker2 kubelet[215]: I0722 02:39:18.822411     215 kubelet_node_status.go:497] "Fast updating node status as it just became ready"
